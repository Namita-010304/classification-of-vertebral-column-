formula,
data = train_data,
hidden = 5,
linear.output = FALSE,
stepmax = 1e6  # To handle convergence for larger datasets
)
# Plot the trained neural network
plot(ann_model)
# Predict on the test dataset
model_results <- compute(ann_model, test_data[, 1:6])  # Use only the feature columns for prediction
# Predict on the test dataset
model_results <- compute(ann_model, test_data[, 1:6])  # Use only the feature columns for prediction
# Extract predictions and round them to binary (0 or 1)
predicted_class <- ifelse(model_results$net.result > 0.5, 1, 0)
# Evaluate performance
actual_class <- test_data$class_output
confusion_matrix <- table(Predicted = predicted_class, Actual = actual_class)
print(confusion_matrix)
str(data)
# Check column names
colnames(data)
#Convert target variable 'class_output' to binary numeric format
# Assuming 'Abnormal' as 1 and 'Normal' as 0 for classification
data$class_output <- ifelse(data$class_output == "Abnormal", 1, 0)
# Normalize numeric features
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
# Apply normalization to all numeric columns except the target ('class_output')
data_norm <- as.data.frame(lapply(data[, 1:6], normalize))  # Select only feature columns (1 to 6)
# Add the target variable back to the normalized data
data_norm$class_output <- data$class_output
# Split the data into training (75%) and testing (25%) sets
set.seed(123)  # For reproducibility
train_index <- sample(1:nrow(data_norm), 0.75 * nrow(data_norm))
train_data <- data_norm[train_index, ]
test_data <- data_norm[-train_index, ]
# Build the formula dynamically based on feature columns
formula <- as.formula(paste("class_output ~", paste(names(data_norm)[1:6], collapse = " + ")))
# Train the ANN model with one hidden layer of 5 neurons
ann_model <- neuralnet(
formula,
data = train_data,
hidden = 5,
linear.output = FALSE,
stepmax = 1e6  # To handle convergence for larger datasets
)
# Plot the trained neural network
plot(ann_model)
# Predict on the test dataset
model_results <- compute(ann_model, test_data[, 1:6])  # Use only the feature columns for prediction
# Extract predictions and round them to binary (0 or 1)
predicted_class <- ifelse(model_results$net.result > 0.5, 1, 0)
# Evaluate performance
actual_class <- test_data$class_output
confusion_matrix <- table(Predicted = predicted_class, Actual = actual_class)
print(confusion_matrix)
# Calculate accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
# Check column names
colnames(data)
#Convert target variable 'class_output' to binary numeric format
# Assuming 'Abnormal' as 1 and 'Normal' as 0 for classification
data$class_output <- ifelse(data$class_output == "Abnormal", 1, 0)
# Normalize numeric features
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
# Apply normalization to all numeric columns except the target ('class_output')
data_norm <- as.data.frame(lapply(data[, 1:6], normalize))  # Select only feature columns (1 to 6)
# Add the target variable back to the normalized data
data_norm$class_output <- data$class_output
# Split the data into training (75%) and testing (25%) sets
set.seed(123)  # For reproducibility
train_index <- sample(1:nrow(data_norm), 0.75 * nrow(data_norm))
train_data <- data_norm[train_index, ]
test_data <- data_norm[-train_index, ]
# Build the formula dynamically based on feature columns
formula <- as.formula(paste("class_output ~", paste(names(data_norm)[1:6], collapse = " + ")))
# Train the ANN model with one hidden layer of 5 neurons
ann_model <- neuralnet(
formula,
data = train_data,
hidden = 5,
linear.output = FALSE,
stepmax = 1e6  # To handle convergence for larger datasets
)
# Plot the trained neural network
plot(ann_model)
# Predict on the test dataset
model_results <- compute(ann_model, test_data[, 1:6])  # Use only the feature columns for prediction
# Extract predictions and round them to binary (0 or 1)
predicted_class <- ifelse(model_results$net.result > 0.5, 1, 0)
# Evaluate performance
actual_class <- test_data$class_output
confusion_matrix <- table(Predicted = predicted_class, Actual = actual_class)
print(confusion_matrix)
# Calculate accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
#Convert target variable 'class_output' to binary numeric format
# Assuming 'Abnormal' as 1 and 'Normal' as 0 for classification
data$class_output <- ifelse(data$class_output == "Abnormal", 1, 0)
# Normalize numeric features
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
# Apply normalization to all numeric columns except the target ('class_output')
data_norm <- as.data.frame(lapply(data[, 1:6], normalize))  # Select only feature columns (1 to 6)
# Add the target variable back to the normalized data
data_norm$class_output <- data$class_output
# Split the data into training (75%) and testing (25%) sets
set.seed(123)  # For reproducibility
train_index <- sample(1:nrow(data_norm), 0.75 * nrow(data_norm))
train_data <- data_norm[train_index, ]
test_data <- data_norm[-train_index, ]
# Build the formula dynamically based on feature columns
formula <- as.formula(paste("class_output ~", paste(names(data_norm)[1:6], collapse = " + ")))
# Train the ANN model with one hidden layer of 5 neurons
ann_model <- neuralnet(
formula,
data = train_data,
hidden = 5,
linear.output = FALSE,
stepmax = 1e6  # To handle convergence for larger datasets
)
# Plot the trained neural network
plot(ann_model)
# Predict on the test dataset
model_results <- compute(ann_model, test_data[, 1:6])  # Use only the feature columns for prediction
# Predict on the test dataset
model_results <- predict(ann_model, test_data[, 1:6])  # Use only the feature columns for prediction
# Extract predictions and round them to binary (0 or 1)
predicted_class <- ifelse(model_results$net.result > 0.5, 1, 0)
# Predict on the test dataset
model_results <- predict(ann_model, test_data[, 1:6])  # Use only the feature columns for prediction
# Extract predictions and round them to binary (0 or 1)
predicted_class <- ifelse(model_results$net.result > 0.5, 1, 0)
# Evaluate performance
actual_class <- test_data$class_output
confusion_matrix <- table(Predicted = predicted_class, Actual = actual_class)
model_results <- compute(ann_model, test_data[, 1:6])  # Use only the feature columns for prediction
model_results <- predict(ann_model, test_data[, 1:6])  # Use only the feature columns for prediction
# Ensure 'model_results' is processed correctly
predicted_class <- ifelse(as.vector(model_results$net.result) > 0.5, 1, 0)
# Evaluate performance
actual_class <- test_data$class_output
confusion_matrix <- table(Predicted = predicted_class, Actual = actual_class)
print(confusion_matrix)
# Calculate accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
# Predict on the test dataset
model_results <- predict(ann_model, test_data[, 1:6])  # Use only the feature columns for prediction
# Process the prediction results correctly
predicted_class <- ifelse(model_results > 0.5, 1, 0)  # Directly threshold the prediction matrix/vector
# Evaluate performance
actual_class <- test_data$class_output
confusion_matrix <- table(Predicted = predicted_class, Actual = actual_class)
print(confusion_matrix)
# Calculate accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
# Load Required Libraries
library(ggplot2)
library(rpart)
library(rpart.plot)
library(neuralnet)
# Load the Dataset
dataset <- read.csv(file.choose())  # Replace with the correct path
dataset$class_output <- as.factor(dataset$class_output)  # Ensure the target is a factor
# Split the Data into Training and Testing Sets
set.seed(42)
indexes <- sample(1:nrow(dataset), 0.7 * nrow(dataset))
train <- dataset[indexes, ]
test <- dataset[-indexes, ]
# Initialize Accuracy Data
model_accuracies <- data.frame(
Model = c("Decision Tree", "ANN", "SVM", "K-Means Clustering"),
Accuracy = c(NA, NA, NA, NA) # Placeholder for accuracies
)
# ------------------------------
# 1. Decision Tree Model
# ------------------------------
target <- class_output ~ pelvic_incidence + lumbar_lordosis_angle + pelvic_radius + degree_spondylolisthesis
tree <- rpart(target, data = train, method = "class")
pred_tree <- predict(tree, test, type = "class")
# Accuracy
conf_matrix_tree <- table(test$class_output, pred_tree)
accuracy_tree <- sum(diag(conf_matrix_tree)) / sum(conf_matrix_tree)
model_accuracies$Accuracy[1] <- accuracy_tree
# ------------------------------
# 2. ANN Model
# ------------------------------
# Normalize Features
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
# Normalize Data (Exclude the Target Variable)
train_norm <- as.data.frame(lapply(train[, -ncol(train)], normalize))
test_norm <- as.data.frame(lapply(test[, -ncol(test)], normalize))
# Add the Target Variable Back
train_norm$class_output <- as.numeric(train$class_output) - 1  # Convert to numeric for ANN
dataset$class_output <- as.factor(dataset$class_output)  # Ensure the target is a factor
# Split the Data into Training and Testing Sets
set.seed(42)
indexes <- sample(1:nrow(dataset), 0.7 * nrow(dataset))
train <- dataset[indexes, ]
test <- dataset[-indexes, ]
# Initialize Accuracy Data
model_accuracies <- data.frame(
Model = c("Decision Tree", "ANN", "SVM", "K-Means Clustering"),
Accuracy = c(NA, NA, NA, NA) # Placeholder for accuracies
)
# ------------------------------
# 1. Decision Tree Model
# ------------------------------
target <- class_output ~ pelvic_incidence + lumbar_lordosis_angle + pelvic_radius + degree_spondylolisthesis
tree <- rpart(target, data = train, method = "class")
pred_tree <- predict(tree, test, type = "class")
# Accuracy
conf_matrix_tree <- table(test$class_output, pred_tree)
accuracy_tree <- sum(diag(conf_matrix_tree)) / sum(conf_matrix_tree)
# Load the Dataset
dataset <- read.csv(file.choose())  # Replace with the correct path
dataset$class_output <- as.factor(dataset$class_output)  # Ensure the target is a factor
# Split the Data into Training and Testing Sets
set.seed(42)
indexes <- sample(1:nrow(dataset), 0.7 * nrow(dataset))
train <- dataset[indexes, ]
test <- dataset[-indexes, ]
# Initialize Accuracy Data
model_accuracies <- data.frame(
Model = c("Decision Tree", "ANN", "SVM", "K-Means Clustering"),
Accuracy = c(NA, NA, NA, NA) # Placeholder for accuracies
)
# ------------------------------
# 1. Decision Tree Model
# ------------------------------
target <- class_output ~ pelvic_incidence + lumbar_lordosis_angle + pelvic_radius + degree_spondylolisthesis
tree <- rpart(target, data = train, method = "class")
pred_tree <- predict(tree, test, type = "class")
# Accuracy
conf_matrix_tree <- table(test$class_output, pred_tree)
accuracy_tree <- sum(diag(conf_matrix_tree)) / sum(conf_matrix_tree)
model_accuracies$Accuracy[1] <- accuracy_tree
# ------------------------------
# 2. ANN Model
# ------------------------------
# Normalize Features
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
# Normalize Data (Exclude the Target Variable)
train_norm <- as.data.frame(lapply(train[, -ncol(train)], normalize))
# ------------------------------
# 3. SVM Model
# ------------------------------
library(e1071)
# Encoding Target Feature
train$class_output <- factor(train$class_output)
test$class_output <- factor(test$class_output)
# Train SVM Model
svm_classifier <- svm(
formula = class_output ~ pelvic_incidence + lumbar_lordosis_angle + pelvic_radius + degree_spondylolisthesis,
data = train,
type = "C-classification",
kernel = "linear"
)
svm_pred <- predict(svm_classifier, newdata = test)
# Accuracy
conf_matrix_svm <- table(test$class_output, svm_pred)
accuracy_svm <- sum(diag(conf_matrix_svm)) / sum(conf_matrix_svm)
model_accuracies$Accuracy[3] <- accuracy_svm
# ------------------------------
# 4. K-Means Clustering
# ------------------------------
library(cluster)
# Clustering
features_case1 <- c("pelvic_incidence", "lumbar_lordosis_angle", "pelvic_radius", "degree_spondylolisthesis")
dataset_case1 <- dataset[, features_case1]
set.seed(240)
kmeans_model <- kmeans(dataset_case1, centers = 3, nstart = 20)
# Confusion Matrix
conf_matrix_kmeans <- table(dataset$class_output, kmeans_model$cluster)
# Adjust Clusters (Matching Predicted Labels to Actual Labels)
map_clusters <- apply(conf_matrix_kmeans, 2, which.max)
pred_kmeans <- map_clusters[kmeans_model$cluster]
# Accuracy
accuracy_kmeans <- mean(pred_kmeans == as.numeric(dataset$class_output))
model_accuracies$Accuracy[4] <- accuracy_kmeans
# ------------------------------
# Plot the Model Accuracies
# ------------------------------
ggplot(data = model_accuracies, aes(x = Model, y = Accuracy, fill = Model)) +
geom_bar(stat = "identity") +
theme_minimal() +
labs(title = "Model Accuracy Comparison (Vertebral Column Dataset)",
x = "Model",
y = "Accuracy") +
geom_text(aes(label = round(Accuracy, 4)), vjust = -0.5, size = 4)
# ------------------------------
# 2. ANN Model
# ------------------------------
# Normalize Features
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
# Normalize Data (Exclude the Target Variable)
train_norm <- as.data.frame(lapply(train[, -ncol(train)], normalize))
# Load Required Libraries
library(ggplot2)
library(rpart)
library(rpart.plot)
library(neuralnet)
library(e1071)
library(class)
# Load the Dataset
data <- read.csv(file.choose())  # Choose the vertebral column dataset
str(data)
# Convert Target Variable 'class_output' to Binary Numeric Format
data$class_output <- ifelse(data$class_output == "Abnormal", 1, 0)
# Initialize Accuracy Data
model_accuracies <- data.frame(
Model = c("Decision Tree", "ANN", "SVM", "K-Means Clustering"),
Accuracy = c(NA, NA, NA, NA) # Placeholder for accuracies
)
# ------------------------------
# 1. Decision Tree Model
# ------------------------------
set.seed(42)
indexes <- sample(1:nrow(data), 0.7 * nrow(data))
train <- data[indexes, ]
test <- data[-indexes, ]
# Decision Tree Model
target <- class_output ~ pelvic_incidence + lumbar_lordosis_angle + pelvic_radius + degree_spondylolisthesis
tree <- rpart(target, data = train, method = "class")
pred_tree <- predict(tree, test, type = "class")
# Accuracy
conf_matrix_tree <- table(test$class_output, pred_tree)
accuracy_tree <- sum(diag(conf_matrix_tree)) / sum(conf_matrix_tree)
model_accuracies$Accuracy[1] <- accuracy_tree
# Load Required Libraries
library(ggplot2)
library(rpart)
library(rpart.plot)
library(neuralnet)
library(e1071)
library(class)
# Load the Dataset
data <- read.csv(file.choose())  # Choose the vertebral column dataset
str(data)
# Convert Target Variable 'class_output' to Binary Numeric Format
data$class_output <- ifelse(data$class_output == "Abnormal", 1, 0)
# Initialize Accuracy Data
model_accuracies <- data.frame(
Model = c("Decision Tree", "ANN", "SVM", "K-Means Clustering"),
Accuracy = c(NA, NA, NA, NA) # Placeholder for accuracies
)
# ------------------------------
# 1. Decision Tree Model
# ------------------------------
set.seed(42)
indexes <- sample(1:nrow(data), 0.7 * nrow(data))
train <- data[indexes, ]
test <- data[-indexes, ]
# Decision Tree Model
target <- class_output ~ pelvic_incidence + lumbar_lordosis_angle + pelvic_radius + degree_spondylolisthesis
tree <- rpart(target, data = train, method = "class")
# Decision Tree Model
target <- class_output ~ pelvic_incidence + lumbar_lordosis_angle + pelvic_radius + degree_spondylolisthesis
tree <- rpart(target, data = train, method = "class")
pred_tree <- predict(tree, test, type = "class")
# Accuracy
conf_matrix_tree <- table(test$class_output, pred_tree)
accuracy_tree <- sum(diag(conf_matrix_tree)) / sum(conf_matrix_tree)
model_accuracies$Accuracy[1] <- accuracy_tree
# ------------------------------
# 2. ANN Model
# ------------------------------
# Normalize Features
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
# Normalize Data (Exclude the Target Variable)
data_norm <- as.data.frame(lapply(data[, 1:6], normalize))  # Select feature columns
data_norm$class_output <- data$class_output
# Split Data
train_index <- sample(1:nrow(data_norm), 0.75 * nrow(data_norm))
train_data <- data_norm[train_index, ]
test_data <- data_norm[-train_index, ]
# Build the ANN Formula Dynamically
formula <- as.formula(paste("class_output ~", paste(names(data_norm)[1:6], collapse = " + ")))
# Train the ANN Model
ann_model <- neuralnet(
formula,
data = train_data,
hidden = 5,  # Single hidden layer with 5 neurons
linear.output = FALSE,
stepmax = 1e6  # Handle convergence issues for larger datasets
)
# Predict
ann_pred <- predict(ann_model, test_data[, 1:6])  # Use feature columns for prediction
predicted_class <- ifelse(ann_pred > 0.5, 1, 0)  # Convert probabilities to binary classification
# Accuracy
conf_matrix_ann <- table(Predicted = predicted_class, Actual = test_data$class_output)
accuracy_ann <- sum(diag(conf_matrix_ann)) / sum(conf_matrix_ann)
model_accuracies$Accuracy[2] <- accuracy_ann
# ------------------------------
# 3. SVM Model
# ------------------------------
train$class_output <- factor(train$class_output)
test$class_output <- factor(test$class_output)
# Train SVM Model
svm_classifier <- svm(
formula = class_output ~ pelvic_incidence + lumbar_lordosis_angle + pelvic_radius + degree_spondylolisthesis,
data = train,
type = "C-classification",
kernel = "linear"
)
svm_pred <- predict(svm_classifier, newdata = test)
# Accuracy
conf_matrix_svm <- table(test$class_output, svm_pred)
accuracy_svm <- sum(diag(conf_matrix_svm)) / sum(conf_matrix_svm)
model_accuracies$Accuracy[3] <- accuracy_svm
# ------------------------------
# 4. K-Means Clustering
# ------------------------------
features_case1 <- c("pelvic_incidence", "lumbar_lordosis_angle", "pelvic_radius", "degree_spondylolisthesis")
dataset_case1 <- data[, features_case1]
set.seed(240)
kmeans_model <- kmeans(dataset_case1, centers = 3, nstart = 20)
# Confusion Matrix
conf_matrix_kmeans <- table(data$class_output, kmeans_model$cluster)
# Adjust Clusters (Matching Predicted Labels to Actual Labels)
map_clusters <- apply(conf_matrix_kmeans, 2, which.max)
pred_kmeans <- map_clusters[kmeans_model$cluster]
# Accuracy
accuracy_kmeans <- mean(pred_kmeans == data$class_output)
model_accuracies$Accuracy[4] <- accuracy_kmeans
# ------------------------------
# Plot the Model Accuracies
# ------------------------------
ggplot(data = model_accuracies, aes(x = Model, y = Accuracy, fill = Model)) +
geom_bar(stat = "identity") +
theme_minimal() +
labs(title = "Model Accuracy Comparison (Vertebral Column Dataset)",
x = "Model",
y = "Accuracy") +
geom_text(aes(label = round(Accuracy, 4)), vjust = -0.5, size = 4)
# ------------------------------
# 4. K-Means Clustering
# ------------------------------
features_case1 <- c("pelvic_incidence", "lumbar_lordosis_angle", "pelvic_radius", "degree_spondylolisthesis")
dataset_case1 <- data[, features_case1]
set.seed(240)
kmeans_model <- kmeans(dataset_case1, centers = 3, nstart = 20)
# Confusion Matrix
conf_matrix_kmeans <- table(data$class_output, kmeans_model$cluster)
# Adjust Clusters (Matching Predicted Labels to Actual Labels)
map_clusters <- apply(conf_matrix_kmeans, 2, which.max)
pred_kmeans <- map_clusters[kmeans_model$cluster]
# Accuracy
accuracy_kmeans <- mean(pred_kmeans == data$class_output)
model_accuracies$Accuracy[4] <- accuracy_kmeans
# ------------------------------
# Plot the Model Accuracies
# ------------------------------
ggplot(data = model_accuracies, aes(x = Model, y = Accuracy, fill = Model)) +
geom_bar(stat = "identity") +
theme_minimal() +
labs(title = "Model Accuracy Comparison (Vertebral Column Dataset)",
x = "Model",
y = "Accuracy") +
geom_text(aes(label = round(Accuracy, 4)), vjust = -0.5, size = 4)
library(cluster)
# Clustering
features_case1 <- c("pelvic_incidence", "lumbar_lordosis_angle", "pelvic_radius", "degree_spondylolisthesis")
dataset_case1 <- dataset[, features_case1]
set.seed(240)
kmeans_model <- kmeans(dataset_case1, centers = 3, nstart = 20)
# Confusion Matrix
conf_matrix_kmeans <- table(dataset$class_output, kmeans_model$cluster)
# Adjust Clusters (Matching Predicted Labels to Actual Labels)
map_clusters <- apply(conf_matrix_kmeans, 2, which.max)
pred_kmeans <- map_clusters[kmeans_model$cluster]
# Accuracy
accuracy_kmeans <- mean(pred_kmeans == as.numeric(dataset$class_output))
model_accuracies$Accuracy[4] <- accuracy_kmeans
# ------------------------------
# Plot the Model Accuracies
# ------------------------------
ggplot(data = model_accuracies, aes(x = Model, y = Accuracy, fill = Model)) +
geom_bar(stat = "identity") +
theme_minimal() +
labs(title = "Model Accuracy Comparison (Vertebral Column Dataset)",
x = "Model",
y = "Accuracy") +
geom_text(aes(label = round(Accuracy, 4)), vjust = -0.5, size = 4)
# Load required library for heatmap
library(ggplot2)
# Create a heatmap for model accuracies
accuracy_matrix <- as.matrix(model_accuracies$Accuracy)  # Convert the accuracy data to matrix format
# Plotting the heatmap
ggplot(data = model_accuracies, aes(x = Model, y = 1, fill = Accuracy)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "blue") +
theme_minimal() +
labs(title = "Model Accuracy Heatmap", x = "Model", y = "") +
geom_text(aes(label = round(Accuracy, 4)), color = "black", size = 6, vjust = 0.5) +
theme(axis.text.y = element_blank(), axis.title.y = element_blank())
